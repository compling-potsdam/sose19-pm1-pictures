{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Vanilla Encoder Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Image Captioning\" is an image captioning encoder-decoder-architecture described in this [tutorial](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "* Identify the main objects in images \n",
    "* Convert an input image into a natural language description\n",
    "* Use encoder-decoder framework:\n",
    "    - The image encoder is a convolutional neural network (CNN) â€“ a resnet-152 model pretrained on the [ILSVRC-2012-CLS](http://image-net.org/challenges/LSVRC/2012/) image classification dataset\n",
    "    - The decoder is a long short-term memory (LSTM) network.\n",
    "    \n",
    "## Dataset\n",
    "\n",
    " * Hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories such as umbrella, dog, balloon)\n",
    " * Test images have no initial annotation (segmentation or labels)\n",
    " * The validation and test data consist of 150,000 photographs, collected from flickr and other search engines, hand labeled with the presence or absence of 1000 object categories\n",
    "\n",
    "## ResNet\n",
    "\n",
    "* 1st place in the ILSVRC 2015 classification competition with top-5 error rate of 3.57%\n",
    "* Train extremely deep neural networks with 150+layers successfully\n",
    "* ResNet-152 achieves 95.51 top-5 accuracies\n",
    "* 2 layer deep (small networks like ResNet 18, 34)\n",
    "* 3 layer deep( ResNet 50, 101, 152)\n",
    "\n",
    "### Skip Connection\n",
    "* Mitigation of the problem of vanishing gradient by allowing this alternate shortcut path for gradient to flow through\n",
    "* Allow the model to learn an identity function which ensures that the higher layer will perform at least as good as the lower layer, and not worse tackling the degradation problem\n",
    "\n",
    "![alt text](./pics/idmap.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Tutorial NN architecture\n",
    "\n",
    "![akt](pics/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Build a vocabulary \n",
    "Code excerpts from build_vocab.py (creating a vocabulary from an annotation caption json file). In the build_vocab.py file there is also a Vocabulary class which is a subclass of object with a __call__ method to get an index of a word and a __len__ method to return the length of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(json, threshold):\n",
    "    # COCO is a large image dataset designed for object detection, segmentation, person keypoints detection,\n",
    "    # stuff segmentation, and caption  |generation.\n",
    "    coco = COCO(json)\n",
    "    # a Counter to keep track on the word occurrences\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        # tokenize the lowercase caption from json file\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        # tracks of how many times equivalent tokens are added\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if (i+1) % 1000 == 0:                                                                                                                 \n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    "\n",
    "    # if the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    # used for padding of sentences\n",
    "    vocab.add_word('<pad>')\n",
    "    # used generate the first word\n",
    "    vocab.add_word('<start>')\n",
    "    # used to end a sentence\n",
    "    vocab.add_word('<end>')\n",
    "    # used as a placeholder for unkown words such as personal names\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize\n",
    "to default 256x256 pixels using antialiasing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> python build_vocab.py   \n",
    "> python resize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass of nn.Module: base class for all neural network modules.\n",
    "class EncoderCNN(nn.Module):\n",
    "    # embed_size: dimension of word embedding vectors, default 256\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        # get the layers of resnet & delete the last fc layer.\n",
    "        modules = list(resnet.children())[:-1]     \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # in_feature is the number of inputs for the linear layer\n",
    "        # applying a linear transformation to the incoming data: y=xA^T+b\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        # add batch normalization layer over a 2d input\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        # sets requires_grad(ient) to False, do not compute gradients\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        # returned features is a numpy array with shape\n",
    "        # simply a list of numbers taken from the output of a neural network layer. \n",
    "        # This vector is a dense representation of the input image, \n",
    "        # and can be used for a variety of tasks such as ranking, classification, or clustering\n",
    "        \n",
    "        # reshape: one shape dimension can be -1: \n",
    "        # in this case, the value is inferred from the length of the array and remaining dimensions.\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        # apply batch normalization\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # add an embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # add an lstm layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # add a liner layer\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # max lenght of a sentence\n",
    "        self.max_seg_length = max_seq_length\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        # concatenate the embeddings and features across the second dim\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        # pad the embeddings\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        # propagate till the output result\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        # add a \"fake\" dimension to the inputs: \n",
    "        # returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            # hiddens: (batch_size, 1, hidden_size)\n",
    "            hiddens, states = self.lstm(inputs, states)     \n",
    "            # outputs:  (batch_size, vocab_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            \n",
    "            # get the best solution \n",
    "            # predicted: (batch_size)\n",
    "            _, predicted = outputs.max(1)                        \n",
    "            sampled_ids.append(predicted)\n",
    "            # propagate result as input for further batch processing      \n",
    "            # inputs: (batch_size, embed_size)\n",
    "            inputs = self.embed(predicted) \n",
    "            # inputs: (batch_size, 1, embed_size)\n",
    "            inputs = inputs.unsqueeze(1) \n",
    "        # sampled_ids: (batch_size, max_seq_length)\n",
    "        # get one tensor from the list of tensors\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                \n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase\n",
    "* For the encoder part, the pretrained CNN extracts the feature vector from a given input image.\n",
    "* The feature vector is linearly transformed to have the same dimension as the input dimension of the LSTM network.\n",
    "* For the decoder part, source and target texts are predefined.\n",
    "* Using these source and target sequences and the feature vector, the LSTM decoder is trained as a language model conditioned on the feature vector.\n",
    "\n",
    "```\n",
    "python train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excerpts from train.py file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[...]\n",
    "# image preprocessing, normalization for the pretrained resnet\n",
    "# normalize the image by the mean and standard deviation of the images's RGB channels\n",
    "# also randomize with crop & flipping of the images \n",
    " transform = transforms.Compose([\n",
    "        transforms.RandomCrop(args.crop_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                            (0.229, 0.224, 0.225))])\n",
    "\n",
    "[...]\n",
    "\n",
    "# build the encoder\n",
    "encoder = EncoderCNN(args.embed_size).to(device)\n",
    "# build the decoder\n",
    "decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)\n",
    "\n",
    "# use cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# get all the parameters to optimize - all of the decoder & the parameters of the last 2 layers of the encoder\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "# Use Adam optimizer to hold the current state and will update the parameters based on the computed gradients\n",
    "optimizer = torch.optim.Adam(params, lr=args.learning_rate)\n",
    "\n",
    "# train the models\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(args.num_epochs):\n",
    "    # fetch the data: the images, their captions and the length of the captions via data_loader\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        # set mini-batch dataset: images, captions, ground-truth\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # get the features from the images\n",
    "        features = encoder(images)\n",
    "        # sample with the decoder and the given features\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        # compute the crossentropy loss, forward\n",
    "        loss = criterion(outputs, targets)\n",
    "        # need zero out gradients before backpropragation so that they can be updated correctly\n",
    "        # without that the gradients are accumulated on subsequent backward passes\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # print log info\n",
    "        if i % args.log_step == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, args.num_epochs, i, total_step, loss.item(), np.exp(loss.item())))\n",
    "\n",
    "        # save the model checkpoints\n",
    "        if (i+1) % args.save_step == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(\n",
    "                    args.model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            torch.save(encoder.state_dict(), os.path.join(\n",
    "                    args.model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test phase\n",
    "* the encoder part is almost same as the training phase : with the difference that batchnorm layer uses moving average and variance instead of mini-batch statistics.\n",
    "* the LSTM decoder canâ€™t see the image description: feeds back the previosly generated word to the next input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[...]\n",
    "# Image preprocessing\n",
    "# Transforms are common image transformations. They can be chained together using Compose\n",
    "# Convert a PIL (Python Imaging Library) Image or numpy.ndarray to tensor.\n",
    "# Normalize a tensor image with mean and standard deviation.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "        (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open(args.vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Build models\n",
    "encoder = EncoderCNN(args.embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Load the trained model parameters\n",
    "encoder.load_state_dict(torch.load(args.encoder_path))\n",
    "decoder.load_state_dict(torch.load(args.decoder_path))\n",
    "\n",
    "# Prepare an image\n",
    "image = load_image(args.image, transform)\n",
    "image_tensor = image.to(device)\n",
    "\n",
    "# Generate features from the images\n",
    "feature = encoder(image_tensor)\n",
    "# Generate an caption from the image\n",
    "sampled_ids = decoder.sample(feature)\n",
    "sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "\n",
    "# Convert word_ids to words\n",
    "sampled_caption = []\n",
    "for word_id in sampled_ids:\n",
    "    word = vocab.idx2word[word_id]\n",
    "    sampled_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "#concatenate the sampled sentence together\n",
    "sentence = ' '.join(sampled_caption)\n",
    "\n",
    "# print the generated caption\n",
    "print (sentence)\n",
    "image = Image.open(args.image)\n",
    "# show the image\n",
    "plt.imshow(np.asarray(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "\n",
    "To just run a pretrained model on some pictures you have to do the following (on a linux machine)\n",
    "```bash\n",
    "git clone https://github.com/pdollar/coco.git\n",
    "cd coco/PythonAPI/\n",
    "make\n",
    "python setup.py build\n",
    "python setup.py install\n",
    "git clone https://github.com/yunjey/pytorch-tutorial.git\n",
    "cd pytorch-tutorial/tutorials/03-advanced/image_captioning/\n",
    "pip install -r requirements.txt\n",
    "chmod +x download.sh #if you need the dataset\n",
    "./download.sh\n",
    "```\n",
    "\n",
    " You can download the pretrained [model](https://www.dropbox.com/s/ne0ixz5d58ccbbz/pretrained_model.zip?dl=0) and the vocabulary [file](https://www.dropbox.com/s/26adb7y9m98uisa/vocap.zip?dl=0). You should extract pretrained_model.zip to ./models/ and vocab.pkl to ./data/.\n",
    " \n",
    " ```bash\n",
    " python sample.py --image='png/umbrella.png' --encoder_path /path/to/encmodel --decoder_path /path/to/decmodel --vocab_path /path/to/vocab\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Testing with the downloaded pretrained model and vocabulary file. These are the produced captions for the testdata:\n",
    "\n",
    "* <start> a group of young men playing a game of soccer on a field . <end>\n",
    "![alt](pics/testdata/football.jpg)\n",
    "* <start> a brown bear standing next to a brown bird . <end>\n",
    "![alt](pics/testdata/lion_ape.jpg)\n",
    "* <start> a close up of a sign with a sign on it <end>\n",
    "![alt](pics/testdata/motivation_resize.jpg)\n",
    "* <start> a large body of water with a bridge in the background <end>\n",
    "![alt](pics/testdata/night.jpg)\n",
    "* <start> a group of people walking down a street next to a building . <end>\n",
    "![alt](pics/testdata/park_night.jpg)\n",
    "* <start> a city street with a lot of cars and buildings . <end>\n",
    "![alt](pics/testdata/street_crossing.jpg)\n",
    "* <start> a large elephant standing next to a baby elephant . <end>\n",
    "![alt](pics/testdata/elephants.jpg)\n",
    "* <start> a couple of sheep standing next to each other . <end>\n",
    "![alt](pics/testdata/drawing_tiger.jpg)\n",
    "* <start> a large white bird flying in the sky . <end>\n",
    "![alt](pics/testdata/dolphins.jpg)\n",
    "* <start> a herd of sheep grazing on a lush green hillside . <end>\n",
    "![alt](pics/testdata/countryside.jpg)\n",
    "* <start> a cat is sitting on a wooden table next to a keyboard . <end>\n",
    "![alt](pics/testdata/cat_pictures.jpg)\n",
    "* <start> a group of people standing around a table with a bunch of bananas . <end>\n",
    "![alt](pics/testdata/basketball.jpg)\n",
    "* <start> a woman holding an umbrella in the rain . <end>\n",
    "![alt](pics/testdata/umbrella.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
